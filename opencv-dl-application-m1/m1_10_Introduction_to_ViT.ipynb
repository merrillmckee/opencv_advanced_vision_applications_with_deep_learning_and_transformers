{"cells":[{"cell_type":"markdown","id":"29d735ae-b7c2-4857-8e4b-b42ec2ea88f4","metadata":{"id":"29d735ae-b7c2-4857-8e4b-b42ec2ea88f4"},"source":["<h1 style=\"font-size:30px;\">Vision Transformers (ViT)</h1>"]},{"cell_type":"markdown","id":"71dafe50-fdc8-4a31-9b20-926d4ce23c33","metadata":{"id":"71dafe50-fdc8-4a31-9b20-926d4ce23c33"},"source":["In the previous module, we revisited the basics by going through PyTorch and CNN classification tasks. Now its time to gradually shift gears and move into advanced computer vision techniques and practices.\n","\n","Until 2020, Convolutional Neural Networks (CNNs) were the dominant architecture in computer vision but they had limitations in capturing long-range dependencies and adapting to other domains. However, the significant strides made by **Transformers**, introduced by Vaswani et al. in Natural Language Processing (NLP) with models like BERT and GPT, paved the way for their application in computer vision.\n","\n","\n","The vision community sought to make use of the Transformer's ability to capture long-range dependencies in visual scenes, resulting in the Vision Transformer (ViT) architecture. The original paper titled **\"[An Image is Worth 16x16 Words: Transformers For Image Recognition At Scale](https://arxiv.org/abs/2010.11929v2)\"** by Dosovitskiy et. al. from the Google Brain team explored the potential of applying Transformers to image classification tasks.\n","\n","\n","<img src = \"https://learnopencv.com/wp-content/uploads/2023/02/image-9-1024x538.png\" width = 600>\n","\n","\n","This pioneering work laid the foundation for vision architectures such as **CLIP, SAM, TrOCR and DINO** (we will encounter a lot of these architectures in future modules).\n","\n","In the upcoming modules we will get hands on these excellent foundational models for various downstream tasks. In this notebook, we will primarily understand the architectural details of vanilla ViT, along with notable derivatives that have emerged since its introduction, up until Q3 of 2024.\n","\n","Tidbit: As of Oct 2024, OmniVec(ViT) is a SOTA classification model achieving **92.4** on the ImageNet dataset. [[Source](https://paperswithcode.com/sota/image-classification-on-imagenet)]"]},{"cell_type":"markdown","id":"9a1c9d7e-e011-4a6d-91cb-03ae4dadf1f4","metadata":{"id":"9a1c9d7e-e011-4a6d-91cb-03ae4dadf1f4"},"source":["## Table of Contents\n","\n","* [Introduction to ViT](#Introduction-to-ViT)\n","* [Internal Workings of a ViT](#Internal-Workings-of-a-ViT)\n","    * [1. Patch Embeddings](#1.-Patch-Embeddings)\n","    * [2. Positional Embedding](#2.-Positional-Embedding)\n","    * [3. Transformer Encoder](#3.-Transformer-Encoder)\n","        * [3.1 Attention](#3.1-Attention)\n","        * [3.2 LayerNorm and Residual Connections](#3.2-LayerNorm-and-Residual-Connections)\n","        * [3.3 Feed Forward Layers](#3.3-Feed-Forward-Layers)\n","    * [4. Classification Head](#4.-Classification-Head)\n","    * [5. Conclusion](#5.-Conclusion)\n","    * [6. References](#6.-References)\n","    * [7. Further Reads](#7.-Further-Reads)"]},{"cell_type":"markdown","id":"aad1bce7-7f8f-455f-a6a0-c5fdf390bc1f","metadata":{"id":"aad1bce7-7f8f-455f-a6a0-c5fdf390bc1f"},"source":["-----------------------------------------------------------------------------------------------------------------------------------"]},{"cell_type":"markdown","id":"6b869bb2-e658-4922-bffa-777eb95a6e52","metadata":{"id":"6b869bb2-e658-4922-bffa-777eb95a6e52"},"source":["## Introduction to ViT"]},{"cell_type":"markdown","id":"d4e622f4-9f2b-48da-8526-d3dd4d5535f9","metadata":{"id":"d4e622f4-9f2b-48da-8526-d3dd4d5535f9"},"source":["Transformers dominates the NLP space, primarily due to their inherent scalability and ability to handle long range dependencies efficiently. Unlike CNNs which process the entire images as local receptive fields through convolutional kernels, Vision Transformers adepts most of its architecture from vanilla Transformers introduced by Vaswani et.al.\n","\n","**At an high level here are few pointers**:\n","\n","In ViTs, an image is divided into 196 non-overlapping patches, each of size 16x16 pixels. These patches are flattened into $N = HW/P^2$ patches, where $H$, $W$ , $P$ are height, width and patch size respectively. The patches are then linearly projected to get N x 1D Vectors. Each patch is now treated like a token. A special **[CLS]** token similar to the BERT architecture is prepended which acts a class representation of all the patches. Additionally, learnable or fixed positional embeddings are added to each patch which helps to retain the spatial relation between patches within an image.\n","\n","The resulting patch embeddings (N + [CLS] ) are passed to a typical Transformer Encoder block having Multi Head Attention (MHA). The output of this is a contextualized representation of the patch embeddings having self attention. Finally, an MLP Head is attached on top of the classification token patch that gives the classification result for the image.\n","\n"]},{"cell_type":"markdown","id":"73d56ccb-8437-49f7-9145-6d9678608540","metadata":{"id":"73d56ccb-8437-49f7-9145-6d9678608540"},"source":["<img src=\"https://uvadlc-notebooks.readthedocs.io/en/latest/_images/vit.gif\" width = 600>\n","\n","Figure Credits: [Phil Wang - lucidrains](https://github.com/lucidrains/vit-pytorch/blob/main/images/vit.gif)"]},{"cell_type":"markdown","id":"10d85462-5dbb-49d5-a534-94cda4013af9","metadata":{"id":"10d85462-5dbb-49d5-a534-94cda4013af9"},"source":["## Internal Workings of a ViT"]},{"cell_type":"markdown","id":"e7bc6c4c-e836-44f9-b6ea-ed2120b1b92b","metadata":{"id":"e7bc6c4c-e836-44f9-b6ea-ed2120b1b92b"},"source":["---------------------------------------------------------------------------------------------------------------------------------\n","\n","Ok, now we have an holistic idea of how a ViT works. Next we will understand the internal workings of a ViT in much more detailed way considering each block and layers. For a more practical understanding throughout this notebook, we will load a `vit_base_patch16_224` model from the popular `timm` (PyTorch Image Models) library. This will help us examine the model parameters, layers and flow.\n","\n","Here `224` refers to the input image size and `16` is spatial dimensions of the patches, i.e., each path will be of 16x16 resolution."]},{"cell_type":"code","execution_count":null,"id":"1ba3c5fa-be2e-4b3c-bc56-8ae032752928","metadata":{"id":"1ba3c5fa-be2e-4b3c-bc56-8ae032752928"},"outputs":[],"source":["# !pip install -q timm"]},{"cell_type":"markdown","id":"177ad160-c7a0-4cb7-9ec6-f4db5c5fed73","metadata":{"id":"177ad160-c7a0-4cb7-9ec6-f4db5c5fed73"},"source":["The following ViT Base model has 12 Encoder blocks and 12 Heads.\n","\n"]},{"cell_type":"code","execution_count":null,"id":"82102402-4e68-44be-98d2-b9d6bbdf3405","metadata":{"scrolled":true,"id":"82102402-4e68-44be-98d2-b9d6bbdf3405","outputId":"5bf183ea-a52b-4d40-f6c0-44ab85a7b7b6"},"outputs":[{"data":{"text/plain":["VisionTransformer(\n","  (patch_embed): PatchEmbed(\n","    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n","    (norm): Identity()\n","  )\n","  (pos_drop): Dropout(p=0.0, inplace=False)\n","  (patch_drop): Identity()\n","  (norm_pre): Identity()\n","  (blocks): Sequential(\n","    (0): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (q_norm): Identity()\n","        (k_norm): Identity()\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","    (1): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (q_norm): Identity()\n","        (k_norm): Identity()\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","    (2): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (q_norm): Identity()\n","        (k_norm): Identity()\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","    (3): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (q_norm): Identity()\n","        (k_norm): Identity()\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","    (4): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (q_norm): Identity()\n","        (k_norm): Identity()\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","    (5): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (q_norm): Identity()\n","        (k_norm): Identity()\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","    (6): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (q_norm): Identity()\n","        (k_norm): Identity()\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","    (7): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (q_norm): Identity()\n","        (k_norm): Identity()\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","    (8): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (q_norm): Identity()\n","        (k_norm): Identity()\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","    (9): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (q_norm): Identity()\n","        (k_norm): Identity()\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","    (10): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (q_norm): Identity()\n","        (k_norm): Identity()\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","    (11): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (q_norm): Identity()\n","        (k_norm): Identity()\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","  )\n","  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","  (fc_norm): Identity()\n","  (head_drop): Dropout(p=0.0, inplace=False)\n","  (head): Linear(in_features=768, out_features=1000, bias=True)\n",")"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["import timm\n","\n","model_path = \"vit_base_patch16_224\"\n","\n","vit_model = timm.create_model(\n","    model_path, pretrained = False) #to avoid downloading weights\n","\n","vit_model.eval()"]},{"cell_type":"markdown","id":"f4d53c96-4e7a-4327-8998-af035c6cf544","metadata":{"id":"f4d53c96-4e7a-4327-8998-af035c6cf544"},"source":["### 1. Patch Embeddings\n","\n","Transformer models process inputs as tokens. If we want to apply transformers to image recognition, the first question that comes into mind is \"what is the equivalent of words in images?\" There are several choices, such as treating each pixel as a token. However, we note that the computational complexity of calculating the attention matrix is $O(N^2)$ where $N$ is the sequence length. If we treat each pixel as a separate word, then assuming a relatively small image size of 100×100, the attention matrix will be of size 10000×10000. This is obviously unmanageable even for the largest GPUs.\n","Additionally, treating individual pixels as tokens would fail to capture the interconnected local features between nieghboring pixels.\n","\n","<img src = https://learnopencv.com/wp-content/uploads/2023/02/image-1-1024x716.png width = 600>"]},{"cell_type":"markdown","id":"698ac17c-533a-4d34-94ab-3150f1be9119","metadata":{"id":"698ac17c-533a-4d34-94ab-3150f1be9119"},"source":["An optimal approach would be to treat patches as tokens and learn the inter-patch representations. For this the image is divided into equal sized 16x16 patches. Thus, a RGB image of size $W$x$H$x$3$ is splitted into patches, each of size $w$x$h$x$3$.\n","\n","For example, an input image of 224x224 ($H$, $W$) will be divided into a total of 14x14 = 196 patches ($N$) by a 16x16 patch ($P$).\n","\n","Then all the patches are flattened as sequence of patches. The embedding layer transforms the patch into a hidden, learned representation of dimension $d_{in}$.\n","\n","In code, this \"patch embedding\" can be achieved using `Conv2d` with a kernel size of `16x16` and a stride of `(16,16)` which effectively creates non-overlapping patches, projecting into a feature or vector space of  $d_{in}$ embed dim.\n","\n","```python\n","(patch_embed): PatchEmbed(\n","    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n","    (norm): Identity()\n","  )\n","```\n","\n","In terms of tensor sizes, assuming a batch size of 1, the input is of size [1, 3, 224, 224]. After patch embedding, the tensor has size [1, 196,  $d_{in}$] where each patch is a 1D vector. For example,  $d_{in}$ = 768 in the base vision transformer model.\n","\n","From now, we will use the notation $d_{\\text{in}}$ and $\\text{emb_dim}$ interchangeably for embedding dimension."]},{"cell_type":"markdown","id":"ce787c91-e268-4c8d-bbca-54f96ca55153","metadata":{"id":"ce787c91-e268-4c8d-bbca-54f96ca55153"},"source":["\n","<img src = \"https://learnopencv.com/wp-content/uploads/2023/02/image-2-1024x578.png\" width=600>"]},{"cell_type":"markdown","id":"4a3f54f6-425c-4841-9c6f-85e2bf2f8afb","metadata":{"id":"4a3f54f6-425c-4841-9c6f-85e2bf2f8afb"},"source":["BERT framed the pre-training task as a classification problem. To let the transformer model perform classification, an extra token called the class token was used. Following this idea, ViT concatenated a **learnable [CLS]** patch token to the beginning of the patch sequence. This classification token that acts as a summary representation for the entire image after passing through the final MLP Head predicting the image class.\n","\n","```python\n","cls_token = nn.Parameter(torch.zeros(1, embed_dim))\n","```\n","\n","In terms of tensor sizes, after adding the class token the resulting tensor is of size [1, 197, 768] where the shape is $[B, \\text{patches}, d_{in}]$.\n","\n","<img src = \"https://learnopencv.com/wp-content/uploads/2023/02/image-3-1024x483.png\" width=600>"]},{"cell_type":"markdown","id":"8572880d-c780-47af-b5a8-a99e05ec0a2f","metadata":{"id":"8572880d-c780-47af-b5a8-a99e05ec0a2f"},"source":["### 2. Positional Embedding\n","\n","> \"**Vision Transformer has much less image-specific inductive bias than\n","CNNs. In CNNs, locality, two-dimensional neighborhood structure, and translation equivariance are\n","baked into each layer throughout the whole model. In ViT, only MLP layers are local and translationally equivariant, while the self-attention layers are global**\"\n","\n","We know that vanilla self-attention mechanism from 2017 does not have any concept of temporal order among its inputs. All patches or words are treated equally. This is a problem since the order of patches and words really matters in both NLP and computer vision. Thus, to allow the transformer to learn to differentiate between patches at different locations, we add something called position embedding to the inputs.\n","\n","There are many kinds of position embeddings in the NLP literature such as the sine/cosine fixed embeddings and learnable embeddings. The sine and cosine functions are applied to alternate tokens and help determine the unique position of the patch in the sequence.\n","\n","\n","Vision transformers work about the same with either of these types. In the below figure, a position embedding is just a learnable parameter. Continuing with our example of images of size 224×224, recall that after concatenating the classification token, the tensor has size **[1, 197, 768]**. We will need to instantiate the position embedding parameter to be of the same size and add the patches and position embedding element-wise. The resulting sequence of vectors is then fed into the transformer model.\n","\n","<img src=\"https://learnopencv.com/wp-content/uploads/2023/02/image-4-1024x334.png\" width=800>\n","\n","\n","Positional embeddings helps to capture the spatial relationship of a patch along its row and column. During pre-training the positional embeddings carry no information about the inter-patch 2D positions, all are learnt from scratch. From paper \"*Vision Transformer can handle arbitrary sequence lengths (up to memory constraints), however, the pre-trained position embeddings may no longer be meaningful* \" which suggests that if we fine-tune on different resolution, the learned position embeddings are no longer meaningful for the new image resolution. However in practice, methods like interpolation of postional embedddings to adjust to new resolution are found effective.\n","\n","\n","The following map from the paper shows after training the patches in the same row and column, as well as nearby patches, have higher cosine similarity compared to those farther apart."]},{"cell_type":"markdown","id":"9bf99dea-9339-41db-9186-1d5d34ca5090","metadata":{"id":"9bf99dea-9339-41db-9186-1d5d34ca5090"},"source":["<img src = \"https://learnopencv.com/wp-content/uploads/2024/10/Positional_Embedding_Cosine_Similarity-ViT-16x16-paper.png\" width = 400>"]},{"cell_type":"markdown","id":"e997d7f7-4437-4bb8-9bff-d8b9425d9cf8","metadata":{"id":"e997d7f7-4437-4bb8-9bff-d8b9425d9cf8"},"source":["### 3. Transformer Encoder\n","\n","The core part of a ViT is the Transformer Encoder with **attention mechanism**. Once the embedding patches are prepared it is fed into a series of encoder blocks where the attention mechanism capture semantic and contextual relationship between input patches. An alignment score is computed which is key part of attention map determining how one patch token attends to other.\n","\n","<img src=\"https://learnopencv.com/wp-content/uploads/2024/10/ViT-MHA.png\"  width=\"700\" >"]},{"cell_type":"markdown","id":"14757333-7bb4-4129-b881-ba05a86eee2d","metadata":{"id":"14757333-7bb4-4129-b881-ba05a86eee2d"},"source":["Each Transformer encoder consists of Norm, MHA and Feed Forward Network layers."]},{"cell_type":"markdown","id":"b3106e83-821f-4e16-8fd5-bad97f04fec7","metadata":{"id":"b3106e83-821f-4e16-8fd5-bad97f04fec7"},"source":["Let's look at one of the encoder block our timm ViT model,\n","```python\n","12 x (0): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (q_norm): Identity()\n","        (k_norm): Identity()\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","\n","```"]},{"cell_type":"markdown","id":"aa61e0f6-148b-4db7-b2cb-2f0e786fa874","metadata":{"id":"aa61e0f6-148b-4db7-b2cb-2f0e786fa874"},"source":["##### 3.1 Attention\n","\n","Each patch embedding is transformed into three matrices namely **Query(Q)**, **Key(K)** and **Value(V)** having shape $[B, \\text{num_patches}, \\text{emb_dim}]$\n","\n","The transformations are achieved using learned weight matrices $W_{Q}$, $W_{K}$, and $W_{V}$ which project the input patch embeddings into the query, key and value matrices.\n","\n","$$Q = XW_Q, \\quad K = XW_K, \\quad V = XW_V$$\n","\n","where $X$ is the input embedding matrix.\n","\n","\n","```python\n","Q = nn.Linear(emb_dim, head_size) # In MHA head size\n","K = nn.Linear(emb_dim, head_size)\n","V = nn.Linear(emb_dim, head_size)\n","```\n","\n","The significant components of Self Attention are:\n","\n","- **Query**: What the patch is trying to learn from other patches? (a question)\n","- **Key**: The information other patches hold that might help to answer the query. (answer relevance)\n","- **Value**: The value holds the actual content from the patches, which is weighted by the attention mechanism to produce the final representation.\n","\n","\n","The following analogy is adapted from Jalammar [[Source](https://jalammar.github.io/illustrated-gpt2/)]:\n","\n","> \"A crude analogy is to think of it like searching through a filing cabinet. The query is like a sticky note with the topic you’re researching. The keys are like the labels of the folders inside the cabinet. When you match the tag with a sticky note, we take out the contents of that folder, these contents are the value vector. Except you’re not only looking for one value, but a blend of values from a blend of folders. Multiplying the query vector by each key vector produces a score for each folder (technically: dot product followed by softmax)\"\n","\n","<img src=\"https://jalammar.github.io/images/gpt2/self-attention-example-folders-scores-3.png\" width=400>\n"]},{"cell_type":"markdown","id":"bab501da-e676-474d-99b9-39e9cf88fb68","metadata":{"id":"bab501da-e676-474d-99b9-39e9cf88fb68"},"source":["**Self Attention**:\n","\n","$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n","\n","where, ${d_k}$ is emb_dim of k vector.\n","\n","\n","In self attention mechanism each patch token attends to every other token in the sequence, helping the model to understand inter-patch relationships.\n","\n","- To calculate attention scores, a dot product is applied to Query(Q) of one token with the Key(K) of all other tokens.\n","$$\\text{Score}(Q, K) = Q K^T$$\n","\n","- To avoid large values and stabilize training, divide the attention scores by ${\\sqrt{d_k}}$\n","\n","$$\\text{Scaled Scores} = \\frac{Q K^T}{\\sqrt{d_k}}$$\n","\n","- This scaled scores are passed through a softmax function to convert them into probability distribution.\n","$$\\text{Attention Weights} = \\text{softmax}\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right)$$\n","\n","- Finally, the attention weights are used to compute a weighted sum of the Value(V) vectors.\n","\n","<img src = \"https://learnopencv.com/wp-content/uploads/2023/01/neural-self-attention-cover-picture-768x576.png\" width=650>\n","\n","**Multi Head Attention**:\n","$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\text{head}_2, \\dots, \\text{head}_h) W_O$$\n","\n","MHA extends self attention mechanism by efficiently running multiple attention heads in parallel. Each head learns different types of relationships between patches or tokens. After computing attention for each head, the results are concatenated and passed through a linear transformation. $W_O$ is the output projection matrix, applied after concatenating the results from all attention heads.\n","\n","```python\n","(proj): Linear(in_features=768, out_features=768, bias=True)\n","```\n","The key point is that the `qkv` linear layer simultaneously computes the `qkv` vectors for all heads in a single linear transformation. In MHA the $\\text{num_heads}$ should be a divisible of $\\text{emb_dim}$. Since ViT-Base has 12 heads, the output size is computed as, $3 \\times \\text{emb_dim}$ $=>$ 3 x 768 = 2304.\n","\n","```python\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (q_norm): Identity()\n","        (k_norm): Identity()\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","```\n","\n","\n","In code,\n","```python\n","scale = emb_dim // num_heads  #d_k\n","q = q * scale\n","attn = q @ k.transpose(-2, -1)\n","attn = attn.softmax(dim=-1)\n","attn = self.attn_drop(attn)\n","x = attn @ v\n","```\n","\n","Attention maps offer a way to visualize how a ViT focuses on different image patches, similar to feature maps in CNN  highlighting key regions in an image. We will discuss this in depth in our upcoming notebook."]},{"cell_type":"markdown","id":"411b1a47-5f22-4678-b58f-a1257f18ade0","metadata":{"id":"411b1a47-5f22-4678-b58f-a1257f18ade0"},"source":["##### 3.2 LayerNorm and Residual Connections\n","\n","Layer normalization, first proposed by the Nobel laureate Professor Geoffrey Hinton’s lab, is a slightly different version of batch normalization. We are all familiar with batch norm in the context of computer vision. However, batch norm cannot be directly applied to recurrent architectures. Moreover, since the mean (μ) and standard deviation (σ) statistics in batch norm are calculated for a mini-batch, the results are dependent on the batch size. As shown in below figure , layer normalization overcomes this problem by calculating the statistics for the neurons in a layer rather than across the mini batch. Thus, each sample in the mini batch gets a different μ and σ, but the mean and std deviation are the same for all neurons in a layer.\n","\n","The thing to note is that for typical model sizes, layer norm is slower than batch norm. Thus, some architectures like DEST (which are designed for speed, but we will not introduce them here), use engineering tricks to use batch norm while keeping the training stable. However, for most widely used vision transformers, layer norm is used and is quite critical for their performance.\n","\n","In ViT, the reorganization of the normalization layers - differing from vanilla transformer architecture supports better gradient flow and eliminates the need for warm-up stage during training.\n","```python\n"," (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n"," ```\n","<img src=\"https://learnopencv.com/wp-content/uploads/2023/02/image-5-1024x405.png\" width=700>\n","\n","\n","Additionally, to prevent vanishing gradient problem, the transformer architecture uses skip connections to add the original input embeddings to the output of the each sublayer."]},{"cell_type":"markdown","id":"755085c1-3920-4108-a9e8-e1ec1b8cfc11","metadata":{"id":"755085c1-3920-4108-a9e8-e1ec1b8cfc11"},"source":["##### 3.3 Feed Forward Layers\n","\n","In the Transformer encoder for each block we also see a MLP layer which is being used. This is a sequential module consisting of:\n","\n","- A linear layer that projects the output of the MHSA layer into higher dimensions ($d_{mlp} > d_{in}$). The ouput dimension of 3072 is determined by MLP ratio (typically 4) multipltied by the hidden dimension.\n","  \n","- An activation layer with GELU activation ($\\text{GELU}(x) = x\\phi(x)$, where $x\\phi(x)$ is the cumulative distribution function of the standard gaussian distribution)\n","- A dropout layer to prevent overfitting\n","- A linear layer to project the output back to the same size as the output of the MHA layer.\n","- Another dropout layer to prevent overfitting.\n","\n","\n","```python\n","(mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","```"]},{"cell_type":"markdown","id":"42f177f3-584b-4fa6-8559-651adfe16600","metadata":{"id":"42f177f3-584b-4fa6-8559-651adfe16600"},"source":["### 4. Classification Head\n","\n","We remarked in the above section on ‘classification token’, a learnable parameter called a classification token is concatenated to the patch embeddings. This token becomes a part of the vector sequence fed into the transformer model and evolves with self-attention. Finally, we attach a small MLP classification head on top of this module and read the classification results from it. This is just a vanilla dense layer with the number of neurons equal to the number of classes in the dataset. So, for example, continuing with our example of dim=768, for imagenet dataset, this layer will take in a vector of size 768 and output 1000 class probabilities.\n","\n","```python\n","(head): Linear(in_features=768, out_features=1000, bias=True)\n","```\n","\n","Note that once we have obtained the classification probabilities from the MLP head on top of the classification token, the outputs from all other patches is IGNORED! This seems quite unintuitive and one may wonder why the classification token is required at all. After all, can’t we average the outputs from all the other tokens and train an MLP on top of that, much like what we do with ResNets? Yes, it is quite possible to do so and it works just as well as the classification token approach. Just note that a different, lower learning rate is required to get this to work.\n","\n","\n"]},{"cell_type":"markdown","id":"56a5b56e-bcef-4aab-ac6a-5b6c5490e5ec","metadata":{"id":"56a5b56e-bcef-4aab-ac6a-5b6c5490e5ec"},"source":["## 5. Conclusion"]},{"cell_type":"markdown","id":"ed476940-b347-4ed0-ab21-92f4764874d9","metadata":{"id":"ed476940-b347-4ed0-ab21-92f4764874d9"},"source":["ViTs can scale effectively with enhanced performance as dataset sizes increase and more computational resources become available. However, these improvements come at the cost of increased parameters and latency. This need for more parameters in ViT-based models is likely due to their lack of the image-specific inductive bias inherent to CNNs. Many resource-constrained applications, such as AR and mobile deployments, still rely heavily on lightweight CNNs such as MobileNet, ESPNet, ShuffleNet, and MNASNet, as they are easy to optimize and integrate with task-specific networks.\n","\n","\n","Successors like Swin Transformers and MobileViT architecture addresses these issues by basically applying “transformers as convolutions; allowing to leverage the merits of both convolutions (versatile and simple training) and transformers (global processing).\n","\n","The success of ViTs has made them an integral part of a wide range of tasks, from traditional computer vision applications and multimodal large language models (MLLMs) to vision-based action models for robotics and more."]},{"cell_type":"markdown","id":"0a659212-0f9f-4608-9845-80e2255f3908","metadata":{"id":"0a659212-0f9f-4608-9845-80e2255f3908"},"source":["## 6. References\n","- [An Image is Worth 16x16 Words](https://arxiv.org/abs/2010.11929v2)\n","- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n","- [HuggingFace Timm Repository](https://github.com/huggingface/pytorch-image-models)\n","- [PyTorch Docs](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html)"]},{"cell_type":"markdown","id":"42f1f0fb-1f47-40c3-8bcb-62e120661e69","metadata":{"id":"42f1f0fb-1f47-40c3-8bcb-62e120661e69"},"source":["## 7. Further Reads\n","\n","- [Transformer Visualizer](https://poloclub.github.io/transformer-explainer/)\n","- [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030)\n","- [DeiT : Training data-efficient image transformers & distillation through attention](https://arxiv.org/abs/2012.12877)\n","- [BEiT : BERT Pre-Training of Image Transformers](https://arxiv.org/abs/2106.08254)\n","- [Mobile ViT](https://arxiv.org/abs/2110.02178)\n","- [LLM Visualization](https://bbycroft.net/llm)"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}